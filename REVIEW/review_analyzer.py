import torch
import torch.nn.functional as F
from transformers import CLIPProcessor, CLIPModel, pipeline
from PIL import Image
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from rembg import remove
import imagehash
from skimage.metrics import structural_similarity as ssim
import cv2

class ReviewAnalyzer:
    """
    ìƒí’ˆ ë¦¬ë·° ë¶„ì„ ëª¨ë¸
    - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì¼ì¹˜ì„± ê²€ì¦
    - ê°ì • ë¶„ì„
    - ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # CLIP ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ìš©)
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_model.to(self.device)
        
        # ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="klue/bert-base",
            tokenizer="klue/bert-base",
            device=0 if torch.cuda.is_available() else -1
        )
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def analyze_review(self, 
                      product_name: str, 
                      product_description: str,
                      review_text: str, 
                      review_image_path: str,
                      product_image_path: str,
                      sentiment_text: str = None) -> dict:
        """
        ìƒí’ˆ ì´ë¯¸ì§€ì™€ ë¦¬ë·° ì´ë¯¸ì§€ì˜ ìœ ì‚¬ë„ (CLIP + pHash + SSIM) + í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„(ê¸/ë¶€ì •)
        """
        try:
            # ë¦¬ë·° ì´ë¯¸ì§€ ë°°ê²½ ì œê±° ì „ì²˜ë¦¬
            review_image_path = self._preprocess_review_image(review_image_path)
            product_image_path = self._preprocess_review_image(product_image_path)
            # 1. CLIP ì˜ë¯¸ì  ìœ ì‚¬ë„
            clip_sim = self._compare_images_clip(product_image_path, review_image_path)
            # 2. pHash êµ¬ì¡°ì  ìœ ì‚¬ë„
            phash_sim = self._compare_images_phash(product_image_path, review_image_path)
            # 3. SSIM êµ¬ì¡°ì  ìœ ì‚¬ë„
            ssim_sim = self._compare_images_ssim(product_image_path, review_image_path)
            # ê°€ì¤‘ í‰ê·  (CLIP 0.2, pHash 0.4, SSIM 0.4)
            final_score = 0.2 * clip_sim + 0.4 * phash_sim + 0.4 * ssim_sim
            confidence_score = final_score
            # í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„(ê¸/ë¶€ì •)
            sentiment = None
            if sentiment_text:
                sentiment = self._simple_sentiment(sentiment_text)
            return {
                "clip_similarity": clip_sim,
                "phash_similarity": phash_sim,
                "ssim_similarity": ssim_sim,
                "final_similarity": final_score,
                "image_similarity": {
                    "score": final_score,
                    "is_matched": final_score > 0.7,
                    "description": self._get_match_description(final_score)
                },
                "confidence": {
                    "score": confidence_score,
                    "level": self._get_confidence_level(confidence_score)
                },
                "sentiment": sentiment,
                "overall_assessment": self._get_overall_assessment_img(final_score, confidence_score, sentiment)
            }
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "error": str(e),
                "clip_similarity": 0.0,
                "phash_similarity": 0.0,
                "ssim_similarity": 0.0,
                "final_similarity": 0.0,
                "image_similarity": {"score": 0.0, "is_matched": False, "description": "ë¶„ì„ ì‹¤íŒ¨"},
                "confidence": {"score": 0.0, "level": "ë‚®ìŒ"},
                "sentiment": None,
                "overall_assessment": "ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    def _compare_images_clip(self, img_path1: str, img_path2: str) -> float:
        """
        CLIP ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1)
        """
        try:
            image1 = Image.open(img_path1).convert('RGB')
            image2 = Image.open(img_path2).convert('RGB')
            inputs1 = self.clip_processor(images=image1, return_tensors="pt").to(self.device)
            inputs2 = self.clip_processor(images=image2, return_tensors="pt").to(self.device)
            with torch.no_grad():
                emb1 = self.clip_model.get_image_features(**inputs1)
                emb2 = self.clip_model.get_image_features(**inputs2)
                emb1 = emb1 / emb1.norm(dim=-1, keepdim=True)
                emb2 = emb2 / emb2.norm(dim=-1, keepdim=True)
                sim = torch.nn.functional.cosine_similarity(emb1, emb2).item()
            return max(0.0, min((sim + 1) / 2, 1.0))
        except Exception as e:
            self.logger.error(f"CLIP ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0

    def _compare_images_phash(self, img_path1: str, img_path2: str) -> float:
        """
        Perceptual Hash(pHash) ê¸°ë°˜ ìœ ì‚¬ë„ (0~1)
        """
        try:
            img1 = Image.open(img_path1).convert('RGB')
            img2 = Image.open(img_path2).convert('RGB')
            hash1 = imagehash.phash(img1)
            hash2 = imagehash.phash(img2)
            # í•´ë°ê±°ë¦¬ â†’ ìœ ì‚¬ë„(1-ì •ê·œí™”)
            sim = 1 - (hash1 - hash2) / len(hash1.hash) ** 2
            return float(sim)
        except Exception as e:
            self.logger.error(f"pHash ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0

    def _compare_images_ssim(self, img_path1: str, img_path2: str) -> float:
        """
        SSIM(Structural Similarity) ê¸°ë°˜ ìœ ì‚¬ë„ (0~1)
        """
        try:
            img1 = cv2.imread(img_path1)
            img2 = cv2.imread(img_path2)
            if img1 is None or img2 is None:
                return 0.0
            img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            img1 = cv2.resize(img1, (256, 256))
            img2 = cv2.resize(img2, (256, 256))
            sim, _ = ssim(img1, img2, full=True)
            return float(sim)
        except Exception as e:
            self.logger.error(f"SSIM ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return 0.0

    def _get_match_description(self, score: float) -> str:
        if score >= 0.8:
            return "ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ë§¤ìš° ì˜ ì¼ì¹˜í•©ë‹ˆë‹¤."
        elif score >= 0.6:
            return "ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ëŒ€ì²´ë¡œ ì¼ì¹˜í•©ë‹ˆë‹¤."
        elif score >= 0.4:
            return "ìƒí’ˆê³¼ ì´ë¯¸ì§€ì˜ ì¼ì¹˜ì„±ì´ ë‚®ìŠµë‹ˆë‹¤."
        else:
            return "ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ì¼ì¹˜í•˜ì§€ ì•Šê±°ë‚˜ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _get_confidence_level(self, confidence: float) -> str:
        if confidence >= 0.8:
            return "ë§¤ìš° ë†’ìŒ"
        elif confidence >= 0.6:
            return "ë†’ìŒ"
        elif confidence >= 0.4:
            return "ë³´í†µ"
        else:
            return "ë‚®ìŒ"

    def _get_overall_assessment_img(self, image_similarity: float, confidence_score: float, sentiment: dict = None) -> str:
        if image_similarity < 0.4:
            return "âš ï¸ ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¦¬ë·° ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
        msg = ""
        if image_similarity >= 0.7:
            msg = f"âœ… ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ì˜ ì¼ì¹˜í•©ë‹ˆë‹¤! | ì‹ ë¢°ë„: {confidence_score:.1%}"
        else:
            msg = f"ìƒí’ˆê³¼ ì´ë¯¸ì§€ê°€ ì–´ëŠ ì •ë„ ìœ ì‚¬í•©ë‹ˆë‹¤. | ì‹ ë¢°ë„: {confidence_score:.1%}"
        if sentiment:
            if sentiment["label"] == "positive":
                msg += " | ğŸ˜Š ê¸ì •ì  ë¦¬ë·°"
            elif sentiment["label"] == "negative":
                msg += " | ğŸ˜ ë¶€ì •ì  ë¦¬ë·°"
        return msg
    
    def _preprocess_review_image(self, image_path: str) -> str:
        """
        ë¦¬ë·° ì´ë¯¸ì§€ì—ì„œ ë°°ê²½ ì œê±° (PNGë¡œ ì €ì¥)
        """
        try:
            img = Image.open(image_path)
            out = remove(img)
            temp_path = image_path.rsplit('.', 1)[0] + '_nobg.png'
            out.save(temp_path)
            return temp_path
        except Exception as e:
            self.logger.warning(f"ë°°ê²½ ì œê±° ì‹¤íŒ¨: {str(e)}")
            return image_path

    def _simple_sentiment(self, text: str) -> dict:
        """
        í…ìŠ¤íŠ¸ ê°ì •ë¶„ì„(ê¸ì •/ë¶€ì •) - huggingface pipeline ì‚¬ìš©, ì ìˆ˜ í¬í•¨
        """
        try:
            result = self.sentiment_analyzer(text)[0]
            label = result["label"].lower()
            score = float(result["score"])
            if "pos" in label or "ê¸ì •" in label:
                return {"label": "positive", "description": "ê¸ì •ì ì¸ ë¦¬ë·°", "score": score}
            else:
                return {"label": "negative", "description": "ë¶€ì •ì ì¸ ë¦¬ë·°", "score": 1-score}
        except Exception as e:
            self.logger.warning(f"ê°ì •ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {"label": "unknown", "description": "ê°ì •ë¶„ì„ ì‹¤íŒ¨", "score": 0.5} 